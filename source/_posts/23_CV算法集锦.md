---
title: CV算法集锦
tags: 
	- cv
categories: "cv"
date: 2018-12-27 11:26:00
updated: 2018-12-27 11:35:00
---


### 1. NMS算法(非极大值抑制算法)

该算法主要目的是筛选出重复框，保留有效框。
主要用在 region proposal阶段以及最后框出的阶段。
假设进行非极大值抑制的输入为2000x20的矩阵，2000表示该图像上框的个数，20表示类别数：
具体步骤如下：
- 1 对2000×20维矩阵中每列按从大到小进行排序（每列表示一类，共20类。同一类可能有多个目标，如上图有两个人）；

- 2 从每列最大的得分建议框开始，分别与该列后面的得分建议框进行IoU计算，若IoU>阈值，则剔除得分较小的建议框，否则认为图像中同一类物体有多个目标；**两个同类的目标的建议框基本不会有重叠(因为两个同类在一张图片中肯定不会有大面积重叠啊)，因此去掉建议框重叠较大的实际上是实现了剔除同一个目标的重叠框**

- 3 从每列次大的得分建议框开始，重复步骤2；

- 4 重复步骤3直到遍历完该列所有建议框；

- 5 遍历完2000×20维矩阵所有列，即所有物体种类都做一遍非极大值抑制；

### 2. Batch Normalization [reference](https://www.cnblogs.com/zhoug2020/p/8258432.html)
- 为什么要进行特征归一化:
  - 归一化后加快了梯度下降求最优解的速度;
  - 归一化有可能提高精度;
- 简单缩放 min-max
- 标准差标准化 z-score 0均值标准化(zero-mean normalization)
   - 经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为
- 非线性归一化 比如log

### 3.动量更新方法
- 传统SGD更新方法。更新速度慢
``` Bash {.line-numbers}
x += -learning_rate * dx
```

- momentum update
``` Bash {.line-numbers}
v += mu * v - learning_rate * dx
x += v
```
> 其中一般的，v初始为0，mu是优化参数，一般初始化参数为0.9，当使用交叉验证的时候，参数mu一般设置成[0.5,0.9,0.95,0.99]，在开始训练的时候，梯度下降较快，可以设置mu为0.5，在一段时间后逐渐变慢了，mu可以设置为0.9、0.99。也正是因为有了“惯性”，这个比SGD会稳定一些。

### 4.CE交叉熵 与 BCE二分类交叉熵
二者是不一样的。参见(CE and BCE)[https://zhuanlan.zhihu.com/p/48078990]

### 5.BP算法
BP算法的整体思路如下：对于每个给定的训练样本，首先进行前向计算，计算出网络中每一层的激活值和网络的输出。对于最后一层(输出层)，我们可以直接计算出网络的输出值与已经给出的标签值(label)直接的差距，我们将这个值定义为残差δ。对于输出层之前的隐藏层L，我们将根据L+1层各节点的加权平均值来计算第L层的残差。
插入一些我个人对BP算法的一点比较容易理解的解释(如有错误请指出)：在反向传播过程中，若第x层的a节点通过权值W对x+1层的b节点有贡献，则在反向传播过程中，梯度通过权值W从b节点传播回a节点。不管下面的公式推导，还是后面的卷积神经网络，在反向传播的过程中，都是遵循这样的一个规律。









